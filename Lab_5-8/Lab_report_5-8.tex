\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{indentfirst}
\usepackage{misccorr}


\usepackage{graphicx}%,vmargin}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.png,.jpg}

\usepackage{mathrsfs}
\usepackage{amsmath,amsthm}
\usepackage{hyperref}
\usepackage{float}

\begin{document}
	\begin{titlepage}
		\begin{center}			
			Санкт-Петербургский политехнический университет\\
			Петра Великого
			\vspace{0.25cm}
			
			Институт прикладной математики и механики
			
			Кафедра «Прикладная математика»
			\vfill
			
			\textbf{Отчёт\\
				по лабораторным работам №5-8\\
				по дисциплине\\
				«Математическая статистика»}\\[5mm]
			\bigskip
		\end{center}
		\vfill
		
		\hfill\begin{minipage}{0.45\textwidth}
			Выполнил студент:
			\vspace{0.2cm}
			
			Кондратьев~Д.~А.\\
			группа: 3630102/70301
		\end{minipage}%
		\bigskip
		
		\hfill\begin{minipage}{0.45\textwidth}
			Проверил:
			\vspace{0.2cm}
			
			к.ф.-м.н., доцент\\
			Баженов Александр Николаевич
		\end{minipage}%
		\vfill
		
		\begin{center}
			Санкт-Петербург\\
			2020 г.
		\end{center}
	\end{titlepage}
	
\tableofcontents{}
\listoffigures
\listoftables

\newpage
\section{Постановка задачи}
	\begin{enumerate}
		\item Сгенерировать двумерные выборки размерами 20, 60, 100 для нормального двумерного распределения $N(x, y, 0, 0, 1, 1, \rho)$.
		
		Коэффициент корреляции $\rho$ взять равным 0, 0.5, 0.9.
		
		Каждая выборка генерируется 1000 раз и для неё вычисляются: среднее значение, среднее значение квадрата и дисперсия коэффициентов корреляции Пирсона, Спирмена и квадрантного коэффициента корреляции.
		
		Повторить все вычисления для смеси нормальных распределений:
		$$f(x, y) = 0.9N(x, y, 0, 0, 1, 1, 0.9) + 0.1N(x, y, 0, 0, 10, 10, -0.9).$$
		Изобразить сгенерированные точки на плоскости и нарисовать эллипс равновероятности.
		
		\item Найти оценки коэффициентов линейной регрессии $y_i = a + bx_i + e_i$, используя 20 точек на отрезке $[-1.8; 2]$ с равномерным шагом равным $0.2$. Ошибку $e_i$ считать нормально распределённой с параметрами $(0, 1)$. В качестве эталонной зависимости взять $y_i = 2 + 2x_i + e_i$. При построении оценок коэффициентов использовать два критерия: критерий наименьших квадратов и критерий наименьших модулей.
		
		Проделать то же самое для выборки, у которой в значения $y_1$ и $y_{20}$ вносятся возмущения $10$ и $-10$.
		
		\item Сгенерировать выборку объёмом 100 элементов для нормального распределения $N(x, 0, 1)$. По сгенерированной выборке оценить параметры $\mu$ и $\sigma$ нормального закона методом максимального правдоподобия. В качестве основной гипотезы $H_0$ будем считать, что сгенерированное распределение имеет вид $N(x, \hat{\mu}, \hat{\sigma})$. Проверить основную гипотезу, используя критерий согласия $\chi^2$. В качестве уровня значимости взять $\alpha = 0.05$. Привести таблицу вычислений $\chi^2$.
		
		Также проверить данную гипотезу на равномерном распределении $U(x, -2, 2)$ при размере выборки ровной 20 элементам.
		
		\item Для двух выборок размерами 20 и 100 элементов, сгенерированных согласно нормальному закону $N(x, 0, 1)$, для параметров положения и масштаба построить асимптотически нормальные интервальные оценки на основе точечных оценок метода максимального правдоподобия и классические интервальные оценки на основе статистик $\chi^2$ и Стьюдента. В качестве параметра надёжности взять $\gamma = 0.95$.
	\end{enumerate}
	
\section{Теория}
	\subsection{Двумерное нормальное распределение}
		Двумерная случайная величина $(X, Y)$ называется распределённой нормально (или просто нормальной), если её плотность вероятности определена формулой:
		\begin{multline}\label{eqn:multivariate_normal}
		N(x, y, \overline{x}, \overline{y}, \sigma_x, \sigma_y, \rho) = \frac{1}{2\pi\sigma_x\sigma_y\sqrt{1 - \rho^2}} \times\\
		\times \exp\left\{
		-\frac{1}{2(1 - \rho^2)} \left[\frac{(x - \overline{x})^2}{\sigma^2_x} - 2\rho\frac{(x - \overline{x})(y - \overline{y})}{\sigma_x \sigma_y} + \frac{(y - \overline{y})^2}{\sigma^2_y}\right]
		\right\}
		\end{multline}
		
		Компоненты $X, Y$ двумерной нормальной случайной величины также распределены нормально с математическими ожиданиями $\overline{x}, \overline{y}$ и средними квадратическими отклонениями $\sigma_x$, $\sigma_y$ соответственно [\ref{Book_1},~с.~133-134].
	
		Параметр $\rho$ называется коэффициентом корреляции.
	
	\subsection{Корреляционный момент(ковариация) и коэффициент корреляции}
		\emph{Корреляционным моментом}, иначе \emph{ковариацией}, двух случайных величин $X$ и $Y$ называется математическое ожидание произведения отклонений этих случайных величин от их математических ожиданий [\ref{Book_1},~с.~141].
	
		\begin{equation}\label{eqn:cor_moment}
		K = cov(X, Y) = M[(X - \overline{x})(Y - \overline{y})]
		\end{equation}
		
		\emph{Коэффициентом корреляции} $\rho$ двух случайных величин X и Y называется отношение их корреляционного момента к произведению их средних квадратических отклонений:
		
		\begin{equation}\label{eqn:cor_coef}
		\rho = \frac{K}{\sigma_x\sigma_y}.
		\end{equation}
		
		\emph{Коэффициент корреляции} --- это нормированная числовая характеристика, являющаяся мерой близости зависимости между случайными величинами
		к линейной [\ref{Book_1},~с.~150].
	
	\subsection{Выборочные коэффициенты корреляции}
		\subsubsection{Выборочный коэффициент корреляции Пирсона}
			Пусть по выборке значений $\{{x_i, y_i}\}^n_1$ двумерной с.в. $(X, Y)$ требуется оценить коэффициент корреляции $\rho$ = $\frac{cov(X, Y)}{\sqrt{DX DY}}$. Естественной оценкой для $\rho$ служит его статистический аналог в виде выборочного коэффициента корреляции, предложенного К.Пирсоном, ---
			\begin{equation}\label{eqn:pears}
			r = \frac{\frac{1}{n} \sum(x_i - \overline{x})(y_i - \overline{y})}{\sqrt{\frac{1}{n} \sum (x_i - \overline{x})^2 \frac{1}{n} \sum(y_i - \overline{y})^2}} = \frac{K}{s_X s_Y}
			\end{equation}
			где $K, s^2_X, s^2_Y$ --- выборочные ковариации и дисперсии с.в. $X$ и $Y$ [\ref{Book_1},~с.~535].
	
		\subsubsection{Выборочный квадрантный коэффициент корреляции}
			Кроме выборочного коэффициента корреляции Пирсона, существуют и другие оценки степени взаимосвязи между случайными величинами. К ним относится \emph{выборочный квадрантный коэффициент корреляции}:
			\begin{equation}\label{eqn:quad}
			r_Q = \frac{(n_1 + n_3) - (n_2 + n_4)}{n}
			\end{equation}
			где $n_1, n_2, n_3 и n_4$ --- количества точке с координатами $(x_i, y_i)$, попавшими
			соответственно в I, II, III и IV квадранты декартовой системы с осями $x' = x - med\:x, y' = y - med\:y$ и с центром в точке с координатами $(med\:x, med\:y)$ [\ref{Book_1},~с.~539].
		
		\subsubsection{Выборочный коэффициент ранговой корреляции Спирмена}
			На практике нередко требуется оценить степень взаимодействия между качественными признаками изучаемого объекта. Качественным называется признак, который нельзя измерить точно, но который позволяет сравнивать изучаемые объекты между собой и располагать их в порядке убывания или возрастания их качества. Для этого объекты выстраиваются в определённом порядке в соответствии с рассматриваемым признаком. Процесс упорядочения называется \emph{ранжированием}, и каждому члену упорядоченной последовательности объектов присваивается ранг, или порядковый номер. Например, объекту с наименьшим значением признака присваивается ранг 1, следующему за ним объекту --- ранг~2, и т.д. Таким образом, происходит сравнение каждого объекта со всеми объектами изучаемой выборки.
	
			Если объект обладает не одним, а двумя качественными признаками --- переменными $X$ и $Y$ , то для исследования их взаимосвязи используют выборочный коэффициент корреляции между двумя последовательностями рангов этих признаков.
			
			Обозначим ранги, соотвествующие значениям переменной $X$, через $u$, а ранги, соотвествующие значениям переменной $Y$, --- через $\upsilon$.
			
			Выборочный коэффициент ранговой корреляции Спирмена определяется как выборочный коэффициент корреляции Пирсона между рангами $u$, $\upsilon$ переменных $X, Y$ :
			\begin{equation}\label{eqn:sperman}
			r_S = \frac{\frac{1}{n} \sum(u_i - \overline{u})(\upsilon_i - \overline{\upsilon})}{\sqrt{\frac{1}{n} \sum (u_i - \overline{u})^2 \frac{1}{n} \sum(\upsilon_i - \overline{\upsilon})^2}},
			\end{equation}
			где $\overline{u} = \overline{\upsilon} = \frac{1 + 2 + .. + n}{n} = \frac{n + 1}{2}$ --- среднее значение рангов [\ref{Book_1},~с.~540-541].	
	\subsection{Простая линейная регрессия}
		\subsubsection{Модель простой линейной регрессии}
			Регрессионную модель описания данных называют \emph{простой линейной регрессией}, если
			\begin{equation}\label{eqn:linear_regression}
			y_i = \beta_0 + \beta_1x_i + \varepsilon_i, \quad i = 1, ... , n,
			\end{equation}
			где $x_1$, ... , $x_n$ --- заданные числа (значения фактора);\\
			$y_1$, ... , $y_n$ --- наблюдаемые значения отклика;\\ $\varepsilon_1$, ... , $\varepsilon_n$ — независимые, нормально распределённые N(0, $\sigma$) с нулевым математическим ожиданием и одинаковой (неизвестной) дисперсией случайные величины (ненаблюдаемые);\\ $\beta_0, \beta_1$ — неизвестные параметры, подлежащие оцениванию.
			
			В модели (\ref{eqn:linear_regression}) отклик $y$ зависит зависит от одного фактора $x$, и весь разброс экспериментальных точек объясняется только погрешностями наблюдений (результатов измерений) отклика $y$. Погрешности результатов измерений $x$ в этой модели полагают существенно меньшими погрешностей результатов измерений $y$, так что ими можно пренебречь [\ref{Book_1},~с.~507].
			
	\subsection{Метод наименьших квадратов}
		При оценивании параметров регрессионной модели используют различные методы. Один из наиболее распрстранённых подходов заключается в следующем: вводится мера (критерий) рассогласования отклика и регрессионной функции, и оценки параметров регрессии определяются так, чтобы сделать это рассогласование наименьшим. Достаточно простые расчётные формулы для оценок получают при выборе критерия в виде суммы квадратов отклонений значений отклика от значений регрессионной функции (сумма квадратов остатков):
		\begin{equation}\label{eqn:least_squares}
		Q(\beta_0, \beta_1) = \sum_{i = 1}^{n} \varepsilon^2_i = \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 \rightarrow \min_{\beta_0, \beta_1}.
		\end{equation}
		Задача минимизации квадратичного критерия (\ref{eqn:least_squares}) носит название задачи \emph{метода наименьших квадратов} (МНК), а оценки $\hat{\beta_0}$, $\hat{\beta_1}$ параметров $\beta_0$, $\beta_1$, реализующие минимум критерия (\ref{eqn:least_squares}), называют МНК-\emph{оценками} [\ref{Book_1},~с.~508].
		
		\subsubsection{Расчётные формулы для МНК-оценок}
			МНК-оценки параметров $\hat{\beta_0}$ и $\hat{\beta_1}$ находятся из условия обращения функции Q($\beta_0$, $\beta_1$) в минимум.
		
			Для нахождения МНК-оценок $\hat{\beta_0}$ и $\hat{\beta_1}$ выпишем необходимые условия экстремума:
		
			\begin{equation}\label{eqn:extremum_cond}
			\begin{cases}
			\frac{\partial Q}{\partial \beta_0} = -2 \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i) = 0,\\
			\frac{\partial Q}{\partial \beta_1} = -2 \sum_{i = 1}^{n} (y_i - \beta_0 - \beta_1 x_i)x_i = 0
			\end{cases}
			\end{equation}
			Далее для упрощения записи сумм будем опускать индекс суммирования. Из системы (\ref{eqn:extremum_cond}) получим
			
			\begin{equation}\label{eqn:extremum_cond_2}
			\begin{cases}
			n\hat{\beta_0} + \hat{\beta_1} \sum x_i = \sum y_i\\
			\hat{\beta_0}\sum x_i + \hat{\beta_1} \sum x^2_i = \sum x_i y_i
			\end{cases}
			\end{equation}
				
			Разделим оба уравнения на $n$ и используя известные статистические обозначения для выборочных первых и вторых начальных моментов
			$$\overline{x} = \frac{1}{n} \sum x_i,\; \overline{y} = \frac{1}{n} \sum y_i,\; \overline{x^2} = \frac{1}{n} \sum x^2_i,\; \overline{xy} = \frac{1}{n} \sum x_iy_i,$$
			получим
			\begin{equation}\label{eqn:extremum_cond_3}
			\begin{cases}
			\hat{\beta_0} + \hat{\beta_1}\overline{x} = \overline{y}\\
			\hat{\beta_0}\overline{x} + \hat{\beta_1} \overline{x^2} = \overline{xy}
			\end{cases}
			\end{equation}	
			откуда МНК-оценку $\hat{\beta_1}$ наклона прямой регрессии находим по формуле
			Крамера
			\begin{equation}\label{eqn:beta_1}
			\hat{\beta_1} = \frac{\overline{xy} - \overline{x} \cdot \overline{y}}{\overline{x^2} - {\overline{x}}^2}
			\end{equation}
			а МНК-оценку $\hat{\beta_0}$ определяем непосредственно из первого уравнения системы (\ref{eqn:extremum_cond_3}):
			\begin{equation}\label{eqn:beta_0}
			\hat{\beta_0} = \overline{y} - \overline{x}\hat{\beta_1}
			\end{equation}
			
	\subsection{Метод наименьших модулей}
		Критерий наименьших модулей --- заключается в минимизации следующей функции:
		\begin{equation} \label{least_abs}
		M(a,b) = \sum\limits_{i=1}^n\vert y_i-ax_i-b\vert\to\min
		\end{equation}
	
	\subsection{Метод максимального правдоподобия}
		Метод максимального правдоподобия --- метод оценивания неизвестного параметра путём максимимзации функции правдоподобия.
		
		\begin{equation}
		\hat{\theta}_{\text{МП}} = \arg \max_\theta L(x_1,x_2,\ldots,x_n,\theta)
		\end{equation}
		где $L$ --- функция правдоподобия, которая представляет собой совместную плотность вероятности независимых случайных величин $x_1,x_2,\ldots,x_n$ и является функцией неизвестного параметра $\theta$.
		\begin{equation}
		L = f(x_1,\theta)\cdot f(x_2,\theta)\cdot\cdots\cdot f(x_n,\theta)
		\end{equation}
		Оценкой максимального правдоподобия будем называть такое значение $\hat{\theta}_{\text{МП}}$ из множества допустимых значений параметра $\theta,$ для которого функция правдоподобия принимает при заданных $x_1, x_2, \ldots, x_n$ максимальное значение.
		
		Тогда при оценивании математического ожидания $m$ и дисперсии $\sigma^2$ нормального распределения $N(m,\sigma)$ получим:
		\begin{equation}
		\ln(L)=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln\left(\sigma^2\right)-\frac{1}{2\sigma^2}\sum\limits_{i=1}^n(x_i-m)^2
		\end{equation}
	
	\subsection{Проверка гипотезы о законе распределения генеральной совокупности. Метод хи-квадрат}
		Разобьём генеральную совокупность на $k$ неперсекающихся подмножеств $\Delta_1, \Delta_2,\ldots, \Delta_k,\;\Delta_i = (a_i,a_{i+1}],$ $p_i = P(X\in\Delta_i),\;i=1,2,\ldots,k\;$ --- вероятность того, что точка попала в $i$-ый промежуток.
		
		Так как генеральная совокупность --- это $\mathbb{R}$, то крайние промежутки будут бесконечными: $\Delta_1 = (-\infty,a_1], \;\Delta_k = (a_k,\infty), \;p_i = F(a_i) - F(a_{i-1})$.
		
		$n_i\;\--$ частота попадания выборочных элементов в $\Delta_i,\;i=1,2,\ldots,k.$
		
		В случае справедливости гипотезы $H_0$ относительно частоты $\frac{n_i}{n}$ при больших $n$ должны быть близки к $p_i,$ значит в качестве меры имеет смысл взять: 
		
		\begin{equation}
		Z = \sum\limits_{i=1}^k\frac{n}{p_i}\left(\frac{n_i}{n}-p_i\right)^2
		\end{equation}
	
		Тогда:
		\begin{equation}
		\chi^2_B=\sum\limits_{i=1}^k\frac{n}{p_i}\left(\frac{n_i}{n}-p_i\right)^2=\sum\limits_{i=1}^k\frac{(n_i-np_i)^2}{np_i}
		\end{equation}
		
		\textbf{Теорема К.Пирсона.} Статистика критерия $\chi^2$ асимптотически распределена по закону $\chi^2$ с $k - 1$ степенями свободы.
		
	\subsection{Доверительные интервалы для параметров нормального распределения}
		Оценкой максимального правдоподобия для математического ожидания  является среднее арифметическое: $\mu=\frac{1}{n}\sum\limits_{i=1}^nx_i.$
		
		Оценка максимального правдоподобия для дисперсии вычисляется по формуле: $\sigma^2 = \frac{1}{n}\sum\limits_{i=1}^n(x_i-\overline{x})^2.$
		
		Доверительным интервалом или интервальной оценкой числовой характеристики или параметра распределения $\theta$ с доверительной вероятностью $\gamma$ называется интервал со случайными границами $(\theta_1,\theta_2),$ содержащий параметр $\theta$ с вероятностью $\gamma$.
		
		Функция распределения Стьюдента:
		\begin{equation}
		T = \sqrt{n-1}\frac{\overline{x}-\mu}{\delta}
		\end{equation}
		
		Функция плотности распределения $\chi^2$:
		\begin{equation}
		f(x) =
		\begin{cases}
		0,&x\leq 0\\
		\frac{1}{2^\frac{n}{2}\Gamma\left(\frac{n}{2}\right)}x^{\frac{n}{2}-1}e^{-\frac{x}{2}},& x>0
		\end{cases}
		\end{equation}
		
		Интервальная оценка математического ожидания:
		\begin{equation}
		P=\left(\overline{x}-\frac{\sigma t_{1-\frac{\alpha}{2}}(n-1)}{\sqrt{n-1}}<\mu<\overline{x}+\frac{\sigma t_{1-\frac{\alpha}{2}}(n-1)}{\sqrt{n-1}}\right) = \gamma,
		\end{equation}
		где $t_{1-\frac{\alpha}{2}}$ --- квантиль распределения Стьюдента порядка $1-\frac{\alpha}{2}$.
		
		Интервальная оценка дисперсии:
		\begin{equation}
		P=\left(\frac{\sigma\sqrt{n}}{\sqrt{\chi^2_{1-\frac{\alpha}{2}}(n-1)}}<\sigma<\frac{\sigma\sqrt{n}}{\sqrt{\chi^2_\frac{\alpha}{2}(n-1)}}\right) = \gamma,
		\end{equation}
		где $\chi_{1-\frac{\alpha}{2}}^2,\;\chi_\frac{\alpha}{2}^2\;$ --- квантили распределения Стьюдента порядков $1-\frac{\alpha}{2}$ и $\frac{\alpha}{2}$ соответственно.
		
	\subsection{Доверительные интервалы для математического ожидания m и среднего квадратического отклонения $\sigma$ произвольного распределения при большом объёме выборки. Асимптотический подход}
		
		Асимптотическая интервальная оценка математического ожидания:
		\begin{equation}
		P = \left(\overline{x}-\frac{\sigma u_{1-\frac{\alpha}{2}}}{\sqrt{n}}<m<\overline{x}+\frac{\sigma u_{1-\frac{\alpha}{2}}}{\sqrt{n}}\right)=\gamma,
		\end{equation}
		где $u_{1-\frac{\alpha}{2}}\;\--$ квантиль нормального распределения $N(x,0,1)$ порядка $1-\frac{\alpha}{2}$.
		\begin{equation}
		\sigma(1 - 0.5u_{1 - \alpha/2} \sqrt{e + 2}/ \sqrt{n}) < \sigma < \sigma(1 + 0.5u_{1 - \alpha/2} \sqrt{e + 2}/ \sqrt{n})
		\end{equation}
	
\section{Реализация}
	Лабораторная работа выполнена на программном языке \emph{Python\;3.8} в среде разработки \emph{Jupyter Notebook\;6.0.3}. В работе использовались следующие пакеты языка \emph{Python}:
	\begin{itemize}
		\item \emph{numpy} --- для генерации выборки и работы с массивами;
		
		\item \emph{matplotlib.pyplot} и \emph{seaborn} --- для построения эллипсов рассеивания и графиков;
		
		\item \emph{tabulate} --- для построения таблиц;
		
		\item \emph{scipy.stats} --- содержит необходимые распределения и критерий $\chi^2$;
		
		\item \emph{scipy.optimize} --- для решения задач оптимизации.
	\end{itemize}
	Ссылка на исходный код лабораторной работы приведена в приложении.
		
\section{Результаты}
	\subsection{Выборочные коэффициенты корреляции}
	\begin{center}
		\begin{table}[H]
			\begin{center}
				\begin{tabular}{|c|c|c|c|}
					\hline
					$\rho=0.0$ (\ref{eqn:cor_coef})& $r$ (\ref{eqn:pears}) & $r_S$ (\ref{eqn:sperman}) & $r_Q$ (\ref{eqn:quad})\\
					\hline
					$E(z)$ & $0.01$ & $0.00$ & $0.00$\\
					\hline
					$E(z^2)$ & $0.05$ & $0.05$ & $0.05$\\
					\hline
					$D(z)$ & $0.0531$ & $0.0520$ & $0.0515$\\
					\hline
					\multicolumn{4}{c}{ } \\
					\hline
					$\rho=0.5$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.49$ & $0.46$ & $0.32$ \\
					\hline
					$E(z^2)$ & $0.27$ & $0.25$ & $0.15$ \\
					\hline
					$D(z)$ & $0.0307$ & $0.0348$ & $0.0480$ \\
					\hline
					\multicolumn{4}{c}{ } \\
					\hline
					$\rho=0.9$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.896$ & $0.866$ & $0.70$ \\
					\hline
					$E(z^2)$ & $0.805$ & $0.755$ & $0.51$ \\
					\hline
					$D(z)$ & $0.0022$ & $0.0048$ & $0.0291$ \\
					\hline					
				\end{tabular}
				\caption{Двумерное нормальное распределение, $n = 20$}
			\end{center}
		\end{table}
		
		\begin{table}[H]
			\begin{center}
				\begin{tabular}{|c|c|c|c|}
					\hline
					$\rho=0.0$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.00$ & $0.00$ & $-0.00$\\
					\hline
					$E(z^2)$ & $0.02$ & $0.02$ & $0.02$\\
					\hline
					$D(z)$ & $0.0172$ & $0.0175$ & $0.0171$\\
					\hline
					\multicolumn{4}{c}{ } \\
					\hline
					$\rho=0.5$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.494$ & $0.47$ & $0.32$\\
					\hline
					$E(z^2)$ & $0.254$ & $0.23$ & $0.12$\\
					\hline
					$D(z)$ & $0.0097$ & $0.0109$ & $0.0147$\\
					\hline
					\multicolumn{4}{c}{ } \\
					\hline
					$\rho=0.9$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.8988$ & $0.883$ & $0.706$\\
					\hline
					$E(z^2)$ & $0.8086$ & $0.781$ & $0.507$\\
					\hline
					$D(z)$ & $0.0007$ & $0.0013$ & $0.0089$\\
					\hline					
				\end{tabular}
				\caption{Двумерное нормальное распределение, $n = 60$}
			\end{center}
		\end{table}
		
		\begin{table}[H]
			\begin{center}
				\begin{tabular}{|c|c|c|c|}
					\hline
					$\rho=0.0$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $-0.001$ & $0.000$ & $-0.00$ \\
					\hline
					$E(z^2)$ & $0.010$ & $0.010$ & $0.01$ \\
					\hline
					$D(z)$ & $0.0098$ & $0.0098$ & $0.0105$ \\
					\hline
					\multicolumn{4}{c}{ } \\
					\hline
					$\rho=0.5$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.496$ & $0.476$ & $0.333$ \\
					\hline
					$E(z^2)$ & $0.251$ & $0.233$ & $0.120$ \\
					\hline
					$D(z)$ & $0.0056$ & $0.0064$ & $0.0089$ \\
					\hline
					\multicolumn{4}{c}{ } \\
					\hline
					$\rho=0.9$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.8995$ & $0.8868$ & $0.712$ \\
					\hline
					$E(z^2)$ & $0.8094$ & $0.7871$ & $0.511$ \\
					\hline
					$D(z)$ & $0.0004$ & $0.0006$ & $0.0051$ \\
					\hline					
				\end{tabular}
				\caption{Двумерное нормальное распределение, $n = 100$}
			\end{center}
		\end{table}
		
		\begin{table}[H]
			\begin{center}
				\begin{tabular}{|c|c|c|c|}
					\hline
					$n = 20$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.783$ & $0.75$ & $0.56$ \\
					\hline
					$E(z^2)$ & $0.622$ & $0.57$ & $0.35$ \\
					\hline
					$D(z)$ & $0.0085$ & $0.0128$ & $0.0396$ \\
					\hline
					\multicolumn{4}{c}{ } \\
					\hline
					$n = 60$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.791$ & $0.768$ & $0.57$ \\
					\hline
					$E(z^2)$ & $0.628$ & $0.594$ & $0.34$ \\
					\hline
					$D(z)$ & $0.0024$ & $0.0033$ & $0.0111$ \\
					\hline
					\multicolumn{4}{c}{ } \\
					\hline
					$n = 100$ & $r$ & $r_S$ & $r_Q$\\
					\hline
					$E(z)$ & $0.789$ & $0.771$ & $0.575$ \\
					\hline
					$E(z^2)$ & $0.624$ & $0.596$ & $0.337$ \\
					\hline
					$D(z)$ & $0.0015$ & $0.0019$ & $0.0063$ \\
					\hline					
				\end{tabular}
				\caption{Смесь нормальных распределений}
			\end{center}
		\end{table}
	\end{center}
	
	\newpage
	\subsection{Эллипсы рассеивания}
	\begin{center}
		\begin{figure}[H]
			\includegraphics[width=\textwidth]{"../Lab_5/pictures/Normal"} 
			\caption[Двумерное нормальное распределение]{Двумерное нормальное распределение}
		\end{figure}
		
		\begin{figure}[h!]
			\includegraphics[width=\textwidth]{"../Lab_5/pictures/NormalMixed"}
			\caption[Смесь нормальных распределений]{Смесь нормальных распределений}
		\end{figure}
	\end{center}

	\subsection{Оценки коэффициентов линейной регрессии}
	\begin{center}
		\begin{figure}[H]
			\includegraphics[width=\textwidth]{"../Lab_6/pictures/Linear_regression"} 
			\caption[Линейная регрессия]{Линейная регрессия}
		\end{figure}
	
		\begin{table}[H]
			\begin{center}
				\begin{tabular}{|c|c|c|}
					\multicolumn{3}{c}{Выборка без возмущений} \\
					\hline
					Критерий & $a$ & $b$\\
					\hline
					МНК & $1.502$ & $1.878$\\
					\hline
					МНМ & $1.709$ & $1.563$\\
					\hline
					\multicolumn{3}{c}{ } \\
					\multicolumn{3}{c}{Выборка c возмущениями} \\
					\hline
					Критерий & $a$ & $b$\\
					\hline
					МНК & $0.394$ & $1.895$\\
					\hline
					МНМ & $1.457$ & $1.564$\\
					\hline					
				\end{tabular}
				\caption{Оценки коэффициентов линейной регрессии}
			\end{center}
		\end{table}
	\end{center}
	
	Введем следующую метрику: $$\rho=\sum(y^*_i-y_i)^2$$
	где $y_i$ --- значение эталонной функции в точке $x_i$, $y^*_i$ --- значение функции в точке $x_i$, полученное путем оценки.
	
	Теперь посчитаем данные метрики для МНК и МНМ:
	\begin{itemize}
		\item  выборка без возмущений: $\rho_\text{МНК} = 7.18,\; \rho_\text{МНM} = 7.33$;
		
		\item  выборка с возмущениями:$\rho_\text{МНК} = 70.06,\; \rho_\text{МНM} = 11.94$.
	\end{itemize}
	
	\subsection{Проверка гипотезы о законе распределения генеральной совокупности. Метод хи-квадрат}
		\subsubsection{Стандартное нормальное распределение}
			Метод максимального правдоподобия:
			$$\hat{\mu} \approx 0.01, \quad \hat{\sigma} \approx 1.02.$$
			
			Критерий согласия $\chi^2$:\\
			Количество промежутков $k = 6$.\\
			Уровень значимости $\alpha = 0.05$.
			\begin{table}[H]
				\begin{center}
					\begin{tabular}{|c|c|c|c|c|c|c|}
						\hline
						$i$ & $\Delta_i$          &   $n_i$ &   $p_i$ &   $np_i$ &   $n_i-np_i$ &   $\frac{(n_i-np_i)^2}{np_i}$ \\
						\hline
						1   & $(-\infty, -1.01]$ & $16$  & $0.1562$ & $15.62$ & $0.38$  & $0.01$ \\
						2   & $(-1.01, -0.37]$   & $17$  & $0.2004$ & $20.04$ & $-3.04$ & $0.46$ \\
						3   & $(-0.37, 0.28]$    & $31$  & $0.2517$ & $25.17$ & $5.83$  & $1.35$ \\
						4   & $(0.28, 0.92]$     & $18$  & $0.2122$ & $21.22$ & $-3.22$ & $0.49$ \\
						5   & $(0.92, 1.56]$     & $10$  & $0.1201$ & $12.01$ & $-2.01$ & $0.34$ \\
						6   & $(1.56, \infty]$   & $8$   & $0.0594$ & $5.94$  & $2.06$  & $0.72$ \\
						$\sum$ & ---                & $100$ & $1.0000$ & $100$   & $0.00$  & $3.36 = \chi^2_B$ \\
						\hline
					\end{tabular}
				\end{center}
				\caption{Вычисление $\chi^2_B$ при проверке гипотизы $H_0$ о нормальном законе распределения N($x, \hat{\mu}, \hat{\sigma}$). $N(x, 0, 1)$}
			\end{table}
	
	
		\subsubsection{Равномерное распределение}
			Метод максимального правдоподобия:
			$$\hat{\mu} \approx -0.07, \quad \hat{\sigma} \approx 1.2.$$
			
			Критерий согласия $\chi^2$:\\
			Количество промежутков $k = 5$.\\
			Уровень значимости $\alpha = 0.05$.
			\begin{table}[H]
				\begin{center}
					\begin{tabular}{|c|c|c|c|c|c|c|}
						\hline
						$i$ & $\Delta_i$          &   $n_i$ &   $p_i$ &   $np_i$ &   $n_i-np_i$ &   $\frac{(n_i-np_i)^2}{np_i}$ \\
						\hline
						1   & $(-\infty, -1.01]$ & $5$  & $0.1562$ & $3.12$ & $1.88$  & $1.13$ \\
						2   & $(-1.01, -0.37]$   & $5$  & $0.2828$ & $5.66$ & $-0.66$ & $0.08$ \\
						3   & $(-0.37, 0.28]$    & $3$  & $0.3200$ & $6.40$ & $-3.40$ & $1.81$ \\
						4   & $(0.92, 1.56]$     & $4$  & $0.1815$ & $3.63$ & $0.37$  & $0.04$ \\
						5   & $(1.56, \infty]$   & $3$  & $0.0594$ & $1.19$ & $1.81$  & $2.77$ \\
						$\sum$ & ---             & $20$ & $1.0000$ & $20$   & $0.00$  & $5.81 = \chi^2_B$ \\
						\hline
					\end{tabular}
				\end{center}
				\caption{Вычисление $\chi^2_B$ при проверке гипотизы $H_0$ о нормальном законе распределения N($x, \hat{\mu}, \hat{\sigma}$). $U(x, -2, 2)$}
			\end{table}
			
	\subsection{Доверительные интервалы для параметров нормального распределения}
		\begin{table}[H]
			\begin{center}
				\begin{tabular}{|c|c|c|}
					\hline
					$n$ & $m$ & $\sigma$\\
					\hline
					$20$ & $(-0.62; 0.28)$ & $(0.73; 1.40)$\\ 
					\hline
					$100$ & $(-0.24; 0.12)$ & $(0.81; 1.07)$\\
					\hline
				\end{tabular}
			\end{center}
			\caption{Доверительные интервалы для параметров нормального распределения}
		\end{table}
		
	\subsection{Доверительные интервалы для параметров произвольного распределения. Асимптотический подход}
		\begin{table}[H]
			\begin{center}				
				\begin{tabular}{|c|c|c|}
					\hline
					$n$ & $m$ & $\sigma$\\
					\hline
					$20$ & $(-0.58; 0.24)$ & $(0.83; 1.09)$\\ 
					\hline
					$100$ & $(-0.24; 0.12)$ & $(0.86; 1.01)$\\
					\hline
				\end{tabular}
			\end{center}
			\caption{Доверительные интервалы для параметров произвольного распределения. Асимптотический подход}
		\end{table}
	
\newpage
\section{Обсуждение}
	\subsection{Выборочные коэффициенты корреляции и эллипсы рассеивания}
		Исходя из полученных результатов можно сделать следующие выводы:
		\begin{itemize}
			\item Верны следующие соотношения для дисперсий выборочных коэффициентов корреляции:
			\begin{itemize}
				\item для двумерного нормального распределения: $r < r_S < r_Q$;
				
				\item для смеси нормальных распределений: $r < r_S < r_Q$.
			\end{itemize}
			
			\item Процент попавших элементов выборки в эллипс рассеивания (95\%-ная доверительная область) примерно равен его теоретическому значению (95\%).
			
			\item При уменьшении корреляции эллипс равновероятности стремится к окружности, а при увеличении --- растягивается.
		\end{itemize}
	
	\subsection{Оценки коэффициентов линейной регрессии}
		Исходя из полученных результатов можно сделать следующие выводы:
		\begin{itemize}
			\item  Критерий наименьших квадратов точнее оценивает коэффициенты линейной регрессии на выборке без возмущений, так как $\rho_\text{МНК}<\rho_\text{МНM}$.
			
			\item Критерий наименьших модулей точнее оценивает коэффициенты линейной регрессии на выборке с возмущениями, так как $\rho_\text{МНМ}<\rho_\text{МНК}$.
			
			\item Критерий наименьших модулей устойчив к редким выбросам по сравнению с критерием наименьших квадратов. Но при этом обладает большей вычислительной сложностью из-за необходимости решения задачи минимизации.
		\end{itemize}

	\subsection{Проверка гипотезы о законе распределения генеральной совокупности. Метод хи-квадрат}
		\subsubsection{Стандартное нормальное распределение}
			Табличное значение квартиля: $\chi^2_{0.95}(5) = 11.07$.
			
			Так как $\chi^2_B < \chi^2_{0.95}(5)$, то можно заключить, что гипотеза $H_0$ о нормальном законе распределения $N(x, \hat{\mu}, \hat{\sigma})$ на уровне значимости $\alpha = 0.05$, согласуется с выборкой.
			
		\subsubsection{Равномерное распределение}
			Табличное значение квартиля: $\chi^2_{0.95}(4) = 9.49$.
			
			Так как $\chi^2_B < \chi^2_{0.95}(4)$, то можно заключить, что гипотезу $H_0$ о нормальном законе распределения $N(x, \hat{\mu}, \hat{\sigma})$ на уровне значимости $\alpha = 0.05$, нельзя опровергнуть. Такое несоответствие действительности можно объяснить довольно малым размером выборки.
			
	\subsection{Доверительные интервалы}
		Исходя из полученных результатов можно сделать следующие выводы:
		\begin{itemize}
			\item Генеральные характеристики ($m = 0$ и $\sigma = 1$) накрываются построенными доверительными интервалами.
			
			\item Лучший результат достигается на выборках большого объема, так как получаемые интервалы получаются меньшей длины.
			
			\item Доверительные интервалы для параметров нормального распределения более надёжны, так как основаны на точном, а не асимптотическом распределении.
		\end{itemize}

\newpage
\section{Литература}
	\begin{enumerate}
		\item \label{Book_1} \textbf{Вероятностные разделы математики.} Учебник для бакалавров технических направлений.//Под ред. Максимова~Ю.Д. --- Спб.: «Иван Федоров», 2001. --- 592 c., илл.
		
		\item Correlation and dependence. URL: \url{https://en.wikipedia.org/wiki/Correlation_and_dependence};
		
		\item Least squares. URL: \url{https://en.wikipedia.org/wiki/Least_squares};
		
		\item Least absolute deviations. URL: \url{https://en.wikipedia.org/wiki/Least_absolute_deviations};
		
		\item Maximum likelihood estimation. URL: \url{https://en.wikipedia.org/wiki/Maximum_likelihood_estimation};
		
		\item Chi-squared test. URL: \url{https://en.wikipedia.org/wiki/Chi-squared_test};
		
		\item Таблица значений $\chi^2$. URL: \url{http://statsoft.ru/home/textbook/modules/sttable.html#chi};
		
		\item Confidence interval. URL: \url{https://en.wikipedia.org/wiki/Confidence_interval}
	\end{enumerate}

\section{Приложение}
	\begin{enumerate}
		\item Код лабораторной №5. URL: \url{https://github.com/DmitriiKondratev/MatStat/blob/master/Lab_5/Lab_5.ipynb};
		
		\item Код лабораторной №6. URL: \url{https://github.com/DmitriiKondratev/MatStat/blob/master/Lab_6/Lab_6.ipynb};
		
		\item Код лабораторной №7. URL: \url{https://github.com/DmitriiKondratev/MatStat/blob/master/Lab_7/Lab_7.ipynb};
		
		\item Код лабораторной №8. URL: \url{https://github.com/DmitriiKondratev/MatStat/blob/master/Lab_8/Lab_8.ipynb};
		
		\item Код общего отчёта №5-8. URL: \url{https://github.com/DmitriiKondratev/MatStat/blob/master/Lab_5-8/Lab_report_5-8.tex}	
	\end{enumerate}
\end{document}